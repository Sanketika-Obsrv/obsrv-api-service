flink:
  namespace: flink
  reinstall_sleep_time: 3
  jobs:
    - name: "PipelineMergedJob"
      release_name: merged-pipeline
      job_manager_url: "http://localhost:8081"
    - name: "MasterDataProcessor"
      release_name: master-data-processor
      job_manager_url: "http://localhost:8081"
    - name: "KafkaConnectorJob"
      release_name: kafka-connector
      job_manager_url: "http://localhost:8081"

commands:
  PUBLISH_DATASET:
    workflow:
      - MAKE_DATASET_LIVE
      - SUBMIT_INGESTION_TASKS
      - START_PIPELINE_JOBS
      - DEPLOY_CONNECTORS
      - CREATE_ALERT_METRIC
      - CREATE_AUDIT_EVENT
  RESTART_PIPELINE:
    workflow:
      - START_PIPELINE_JOBS
  RESTART_CONNECTORS:
    workflow:
      - DEPLOY_CONNECTORS

alert_manager:
  metrics:
    - flink:
      - metric: "flink_taskmanager_job_task_operator_ExtractorJob_dataset_id_failed_event_count"
        alias: "Number of Failed Extraction Events"
        description: "This alert tracks how many events failed the extraction stage"
        frequency: 5m
        interval: 5m
        operator: "gt"
        threshold: 100
      - metric: "flink_taskmanager_job_task_operator_ExtractorJob_dataset_id_duplicate_extraction_count"
        alias: "Number of Duplicate Extraction Events"
        description: "This alert tracks how many duplicate events were found during extraction stage"
        frequency: 5m
        interval: 5m
        operator: "gt"
        threshold: 100
      - metric: "flink_taskmanager_job_task_operator_PipelinePreprocessorJob_dataset_id_failed_event_count"
        alias: "Number of Failed Preprocessing Events"
        description: "This alert tracks how many events failed the preprocessing stage"
        frequency: 5m
        interval: 5m
        operator: "gt"
        threshold: 100
      - metric: "flink_taskmanager_job_task_operator_PipelinePreprocessorJob_dataset_id_duplicate_event_count"
        alias: "Number of Duplicate Preprocessing Events"
        description: "This alert tracks how many duplicate events were found during preprocessing stage"
        frequency: 5m
        interval: 5m
        operator: "gt"
        threshold: 100
      - metric: "flink_taskmanager_job_task_operator_PipelinePreprocessorJob_dataset_id_validation_failed_event_count"
        alias: "Number of Failed Validation Events"
        description: "This alert tracks how many events failed the validation stage"
        frequency: 5m
        interval: 5m
        operator: "gt"
        threshold: 100
  object_connector_metrics:
    - metric: "sum_over_time(ObjectDiscoveryJob_cloud_authentication_failure{datasetId='dataset_id'}[1h])"
      alias: "Cloud Authentication Failure"
      description: "This alert tracks whether the cloud authentication failed for the dataset"
      frequency: 1h
      interval: 1h
      operator: "gt"
      threshold: 1
    - metric: "sum_over_time(ObjectProcessorJob_object_tag_update_failure{datasetId='dataset_id'}[1h])"
      alias: "Update Tag Failure"
      description: "This alert tracks whether the tag update failed for the dataset"
      frequency: 1h
      interval: 1h
      operator: "gt"
      threshold: 1
    - metric: "sum_over_time(ObjectDiscoveryJob_num_new_objects{datasetId='dataset_id'}[1d])"
      alias: "Number Of New Objects"
      description: "This alert tracks whether any new objects are processed. If not, then this alert is fired"
      frequency: 1d
      interval: 1h
      operator: "eq"
      threshold: 0
  jdbc_connector_metrics:
    - metric: "sum_over_time(JDBCConnectorJob_failure_count{datasetId='dataset_id'}[1d])"
      alias: "Number of Failed Records"
      description: "This alert tracks whether any records failed to be ingested into the database"
      frequency: 1d
      interval: 1h
      operator: "gt"
      threshold: 10
  masterdata_metrics:
    - metric: "MasterDataProcessorIndexerJob_failure_dataset_count{datasetId='dataset_id'}"
      alias: "Master Dataset failed to index"
      description: "This alert tracks whether the given master dataset failed to index data"
      frequency: 240m
      interval: 5m
      operator: "gt"
      threshold: 0

postgres:
  db_host: localhost
  db_port: 5432
  db_user: postgres
  db_password: postgres
  database: obsrv

config_service:
  host: localhost
  port: 4000

druid:
  router_host: localhost
  router_port: 8888
  supervisor_endpoint: indexer/v1/supervisor

helm_charts_base_dir: ../helm-charts

spark:
  host: "http://spark-master-svc.spark.svc.cluster.local:8998/batches/"
  master:
    host: "spark://spark-master-svc.spark.svc.cluster.local:7077"
  driver:
    extraJavaOptions: "-Dlog4j.configuration=file:/opt/bitnami/spark/conf/log4j.properties"
  executor:
    extraJavaOptions: "-Dlog4j.configuration=file:/opt/bitnami/spark/conf/log4j.properties"

connector_job:
  jdbc:
    - release_name: jdbc-connector
      jar: https://obsrv-connectors.s3.us-east-2.amazonaws.com/jdbc/jdbc-connector-1.0-jar-with-dependencies.jar
      class: org.sunbird.obsrv.job.JDBCConnectorJob
      schedule: "0 * * * *" # Every hour
      args:
        - "/opt/bitnami/spark/conf/jdbc-connector.conf"
  object:
    - release_name: object-discovery
      jar: https://obsrv-connectors.s3.us-east-2.amazonaws.com/object-store/object-discovery-1.0.0.jar
      class: in.sanketika.obsrv.job.ObjectDiscoveryJob
      schedule: "0 1 * * *" # Daily Midnight 1AM
      args:
        - /opt/bitnami/spark/conf/object-discovery.conf
    - release_name: object-processor
      jar: https://obsrv-connectors.s3.us-east-2.amazonaws.com/object-store/object-processor-1.0.0.jar
      class: in.sanketika.obsrv.job.ObjectProcessor
      schedule: "30 * * * *" # Every hour at 30 minutes
      args:
        - /opt/bitnami/spark/conf/object-processor.conf

masterdata_job:
  - release_name: masterdata-indexer
    jar: https://obsrv-connectors.s3.us-east-2.amazonaws.com/masterdata-indexer/data-products-1.0.0.jar
    class: org.sunbird.obsrv.dataproducts.MasterDataProcessorIndexer
    schedule: "0 0 * * *" # Daily Midnight 12AM
    args:
      - /opt/bitnami/spark/conf/masterdata-indexer.conf

kafka:
  brokers: localhost:9092
  telemetry:
    topic: system.telemetry.events

connector_jobs:
  spark:
    namespace: spark
    base_helm_chart: spark-connector-cron
  flink:
    namespace: flink
    base_helm_chart: flink-connector

connector_registry:
  download_path: /tmp/connector-registry
  metadata_file_name: "metadata.json"
  ui_spec_file: "ui-config.json"

backups:
  provider: aws
  bucket_name: obsrv-backups

prometheus:
  pod: prometheus-kube-prometheus-stack-prometheus-0
  namespace: monitoring
  host: http://localhost:9090
  endpoint: /api/v1/admin/tsdb/snapshot
  backup_prefix: prometheus_backups

dataset_api:
  host: "http://localhost:3000"
  pre_signed_url: "v2/files/generate-url"