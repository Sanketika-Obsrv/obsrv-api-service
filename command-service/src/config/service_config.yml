flink:
  namespace: flink
  reinstall_sleep_time: 3
  jobs:
    - name: "PipelineMergedJob"
      release_name: merged-pipeline
      job_manager_url: "http://localhost:8081"
    - name: "MasterDataProcessor"
      release_name: master-data-processor
      job_manager_url: "http://localhost:8081"
    - name: "KafkaConnectorJob"
      release_name: kafka-connector
      job_manager_url: "http://localhost:8081"

commands:
  PUBLISH_DATASET:
    workflow:
      - CREATE_KAFKA_TOPIC
      - MAKE_DATASET_LIVE
      - SUBMIT_INGESTION_TASKS
      - START_PIPELINE_JOBS
      - DEPLOY_CONNECTORS
      - CREATE_AUDIT_EVENT
  RESTART_PIPELINE:
    workflow:
      - START_PIPELINE_JOBS
  RESTART_CONNECTORS:
    workflow:
      - DEPLOY_CONNECTORS

postgres:
  db_host: localhost
  db_port: 5432
  db_user: postgres
  db_password: postgres
  database: obsrv

druid:
  router_host: http://localhost
  router_port: 8888
  supervisor_endpoint: indexer/v1/supervisor

helm_charts_base_dir: ../helm-charts

spark:
  host: "http://spark-master-svc.spark.svc.cluster.local:8998/batches/"
  master:
    host: "spark://spark-master-svc.spark.svc.cluster.local:7077"
  driver:
    extraJavaOptions: "-Dlog4j.configuration=file:/opt/bitnami/spark/conf/log4j.properties"
  executor:
    extraJavaOptions: "-Dlog4j.configuration=file:/opt/bitnami/spark/conf/log4j.properties"

connector_job:
  jdbc:
    - release_name: jdbc-connector
      jar: https://obsrv-connectors.s3.us-east-2.amazonaws.com/jdbc/jdbc-connector-1.0-jar-with-dependencies.jar
      class: org.sunbird.obsrv.job.JDBCConnectorJob
      schedule: "0 * * * *" # Every hour
      args:
        - "/opt/bitnami/spark/conf/jdbc-connector.conf"
  object:
    - release_name: object-discovery
      jar: https://obsrv-connectors.s3.us-east-2.amazonaws.com/object-store/object-discovery-1.0.0.jar
      class: in.sanketika.obsrv.job.ObjectDiscoveryJob
      schedule: "0 1 * * *" # Daily Midnight 1AM
      args:
        - /opt/bitnami/spark/conf/object-discovery.conf
    - release_name: object-processor
      jar: https://obsrv-connectors.s3.us-east-2.amazonaws.com/object-store/object-processor-1.0.0.jar
      class: in.sanketika.obsrv.job.ObjectProcessor
      schedule: "30 * * * *" # Every hour at 30 minutes
      args:
        - /opt/bitnami/spark/conf/object-processor.conf

masterdata_job:
  - release_name: masterdata-indexer
    jar: https://obsrv-connectors.s3.us-east-2.amazonaws.com/masterdata-indexer/data-products-1.0.0.jar
    class: org.sunbird.obsrv.dataproducts.MasterDataProcessorIndexer
    schedule: "0 0 * * *" # Daily Midnight 12AM
    args:
      - /opt/bitnami/spark/conf/masterdata-indexer.conf

kafka:
  brokers: localhost:9092
  telemetry:
    topic: system.telemetry.events
  replication_factor: 1
  no_of_partitions: 1

connector_jobs:
  spark:
    namespace: spark
    base_helm_chart: spark-connector-cron
  flink:
    namespace: flink-connectors
    base_helm_chart: flink-connector

connector_registry:
  download_path: /tmp/connector-registry
  metadata_file_name: "metadata.json"
  ui_spec_file: "ui-config.json"

backups:
  provider: aws
  bucket_name: obsrv-backups

prometheus:
  pod: prometheus-kube-prometheus-stack-prometheus-0
  namespace: monitoring
  host: http://localhost:9090
  endpoint: /api/v1/admin/tsdb/snapshot
  backup_prefix: prometheus_backups

node_selector:

container_security_context:

pod_security_context:
